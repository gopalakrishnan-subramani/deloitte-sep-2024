{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10c4fcff-e713-49b9-9fd7-001e8f4f3791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install drivers/jars while notebook start\n",
    "import os\n",
    "\n",
    "SCALA_VERSION = '2.12'\n",
    "SPARK_VERSION = '3.5.3'\n",
    "\n",
    "# download jar files at runtime, load the jar file\n",
    "\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = f'--packages org.apache.spark:spark-sql-kafka-0-10_{SCALA_VERSION}:{SPARK_VERSION} pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3fb9faf-7a0f-43a1-81b7-d6c9555dfb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc # in jupyter, sc is not initialized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1811ccff-1c5c-49d4-ac42-6742512700a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m \u001b[38;5;66;03m# is not initilaized by default\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark # is not initilaized by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9b08d00-28f2-470b-b6dc-b27a5557b83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local cluster\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0018a53c-6829-46ad-973f-25c8cdda7b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0383b045-3973-40f9-bfb4-9f4b4ef676d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# * means paralleism , it will take from number vritual cores in your laptop\n",
    "spark = SparkSession.builder.master(\"local[*]\")\\\n",
    "                            .config('spark.sql.shuffle.partitions', 4)\\\n",
    "                            .appName(\"SparkStreamingKafkaInvoiceStream\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "731e3d72-b881-4af9-afae-949f92a24759",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://training-sh:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkStreamingKafkaInvoiceStream</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d429b8cca0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "803fd331-651a-45c9-b1e9-94daac8bcb13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TOPIC = 'quickstart-events'\n",
    "# you need kafka jars to support format('kafka')\n",
    "# databricsk already include kafka driver, by spark download does not\n",
    "kafkaDf = spark.readStream.format(\"kafka\")\\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "  .option(\"subscribe\", TOPIC)\\\n",
    "  .load()\n",
    "\n",
    "# .show/print will not work directily due to stream..\n",
    "# linesDf.show() # won't work, show is for batch, read once, process once and output once\n",
    "kafkaDf.printSchema() # works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a2d4dd5-3c0a-44b4-994a-3206b879abf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# key is kafka key, in binary format\n",
    "# value is kafka value, in binary format\n",
    "# topic string\n",
    "# parition, integer\n",
    "# offer long \n",
    "# timestamp - longint in ms\n",
    "# timestampType - Source Time, Record write time\n",
    "\n",
    "# now convert kafka value which is in bytes to STRING, we ignore the key for now...\n",
    "# now we pick only value from the stream..\n",
    "linesDf = kafkaDf.selectExpr(\"timestamp\", \"CAST(value AS STRING)\")\n",
    "linesDf.printSchema() # we get only value as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89a31d05-6a9b-45d4-b346-cd6f2f881ea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- word: string (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split the lines into words, then convert the words into individual row using a function called explode\n",
    "# explode will convert columns/array elements into spark record\n",
    "import pyspark.sql.functions as F\n",
    "# input: welcome to spark\n",
    "# linesDf.value is a column\n",
    "# split convert to list of words [welcome, to, spark]  record\n",
    "# convert list of words into individual word/record\n",
    "# explode, will convert elements into record\n",
    "#wordsDf = linesDf.select(F.split(linesDf.value,\" \"))\n",
    "# explode is same as flatMap in rdd, it flatten the list into elements\n",
    "# it creates records for each elemenet in the input list\n",
    "# after explode the output would be, column name is shown as col\n",
    "#        welcome\n",
    "#        to\n",
    "#        spark\n",
    "# wordsDf = linesDf.select(F.explode(F.split(linesDf.value,\" \")) )\n",
    "\n",
    "wordsDf = linesDf.select(\"timestamp\", F.explode(F.split(linesDf.value,\" \")).alias(\"word\") )\n",
    "#        welcome\n",
    "#        to\n",
    "#        spark\n",
    "# now the same result with col name word\n",
    "\n",
    "\n",
    "wordsDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96fdb1cf-4353-43ce-9c8f-c72fd40ff275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate running word count from stream using WINDOW of 1 MINUTE\n",
    "# if we send the words, for every minute, the count shall reset\n",
    "# \"word\" is a column name\n",
    "# count is aggregate, kindly, we don't apply window here\n",
    "# since no window, count from begingging, continue without any time constraint\n",
    "# and continue without any any time bound, NO WINDOW\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# non-window, the output is word and count\n",
    "# wordCountsDf = wordsDf.groupBy(\"word\").count()\n",
    "# window example, the output schema would differ, start time, end time, word, count\n",
    "wordCountsDf = wordsDf.groupBy(F.window(\"timestamp\", \"1 minute\"), \"word\").count()\n",
    "\n",
    "# show does not work on streams\n",
    "# we have to print result in a stream manner.\n",
    "# .format(\"console\") - console meands stdout\n",
    "# mode is complete - prints whole data, for a single word, it prints whole results\n",
    "# word may be apple, but the output will include all other words\n",
    "# in batch, we use .write.format, one time write\n",
    "# in stream, we use .writeSteam.format (continuous write)\n",
    "# we did not use trigger , spark has default trigger 300 ms\n",
    "# use trigger api to define, this is not Window\n",
    "# trigger is for micro batch - collect data until trigger time reaches and process data\n",
    "# to print the data on console..\n",
    "# read the data send by nc command from linux terminal, print it on Jupyter console\n",
    "# the output shall be printed on spark compute cluster logs, not on notebook cell\n",
    "# notice that stream will start as job. and the stream shall be keep running until we intrupt\n",
    "echoOnconsole = wordCountsDf\\\n",
    "                .writeStream\\\n",
    "                .outputMode(\"complete\")\\\n",
    "                .format(\"console\")\\\n",
    "                .option(\"checkpointLocation\", \"file:///c:/tmp/spark/FileStore/tables/tmp/spark-wordcount-output\") \\\n",
    "                .start() # start the query. spark will subscribe for data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f33641-77ad-46cb-9c64-395e67291bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
